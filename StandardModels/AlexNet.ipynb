{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24ceef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\ameyv\\anaconda3\\lib\\site-packages (1.6.5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import random\n",
    "import scipy\n",
    "import cv2\n",
    "\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f3adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d35e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AlexNet                                  --                        --\n",
       "├─Sequential: 1-1                        [1, 256, 6, 6]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n",
       "│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n",
       "│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n",
       "│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n",
       "│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n",
       "│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n",
       "│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n",
       "│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n",
       "│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n",
       "│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n",
       "│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n",
       "├─Sequential: 1-3                        [1, 1000]                 --\n",
       "│    └─Dropout: 2-14                     [1, 9216]                 --\n",
       "│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n",
       "│    └─ReLU: 2-16                        [1, 4096]                 --\n",
       "│    └─Dropout: 2-17                     [1, 4096]                 --\n",
       "│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n",
       "│    └─ReLU: 2-19                        [1, 4096]                 --\n",
       "│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n",
       "==========================================================================================\n",
       "Total params: 61,100,840\n",
       "Trainable params: 61,100,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 714.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 3.95\n",
       "Params size (MB): 244.40\n",
       "Estimated Total Size (MB): 248.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_alexnet, (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c06f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_dir = r\"C:\\Users\\ameyv\\BTP\\val\"\n",
    "# transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()])\n",
    "# val_data = datasets.ImageFolder(val_data_dir, transform=transform)\n",
    "# dataloader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1df0eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,df,img_folder,transform):\n",
    "        self.df=df\n",
    "        self.transform=transform\n",
    "        self.img_folder=img_folder\n",
    "        self.image_names=self.df[:]['name']\n",
    "        self.labels=self.df[:]['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        number_str = str(index)\n",
    "        zero_filled_number = str(number_str.zfill(5))\n",
    "        file_base = '\\ILSVRC_2010_val_000'\n",
    "        image=cv2.imread(self.img_folder + file_base + zero_filled_number +'.JPEG')\n",
    "        image=self.transform(image)\n",
    "        targets=self.labels[index]\n",
    "        sample = {'image': image,'labels':targets}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6599cc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000019C8F3A7EE0>\n"
     ]
    }
   ],
   "source": [
    "test_set = r\"C:\\Users\\ameyv\\BTP\\labels.csv\"\n",
    "img_folder = r\"C:\\Users\\ameyv\\BTP\\val\"\n",
    "df = pd.read_csv(test_set)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToPILImage(),transforms.CenterCrop(224), transforms.ToTensor()])\n",
    "\n",
    "test_dataset=ImageDataset(df,img_folder,test_transform)\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "print(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8499b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 46537\n",
    "# number_str = str(index)\n",
    "# zero_filled_number = str(number_str.zfill(5))\n",
    "# file_base = \"\\ILSVRC_2010_val_000\"\n",
    "# inp_image=cv2.imread(img_folder + zero_filled_number +\".JPEG\")\n",
    "\n",
    "def imshow(inp_image, title=None):\n",
    "    \"\"\"imshow for Tensor.\"\"\"\n",
    "    inp_image = inp_image.numpy().transpose((1, 2, 0))\n",
    "    inp_image = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
